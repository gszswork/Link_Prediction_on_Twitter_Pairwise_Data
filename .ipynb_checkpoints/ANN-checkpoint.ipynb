{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.0\n",
      "0    0.0\n",
      "0    0.0\n",
      "0    0.0\n",
      "0    0.0\n",
      "    ... \n",
      "0    1.0\n",
      "0    1.0\n",
      "0    1.0\n",
      "0    1.0\n",
      "0    1.0\n",
      "Name: 2, Length: 200000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# pre\n",
    "data_dir = \"data/\"\n",
    "def save_obj(obj, name ):\n",
    "    with open( name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(data_dir + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "\n",
    "test_df = load_obj(\"test_df\")\n",
    "\n",
    "# Get the training_data_df\n",
    "training_data_df = load_obj(\"training_df_20w_ra\")\n",
    "final_training_data_df = training_data_df.iloc[:,3:20]\n",
    "# 0.511756 0.028259\n",
    "# get the labeled data df\n",
    "final_labels_df = training_data_df.iloc[:,2]\n",
    "print(final_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0          1         2         3         4        5         6  \\\n",
      "0   3563811.0  3600160.0  0.000000  0.000000  0.000000    667.0  0.000000   \n",
      "0   2052043.0  1401960.0  0.000000  0.000000  0.000000    666.0  0.000000   \n",
      "0   4517994.0  1690636.0  0.626606  0.000692  0.011152   4335.0  0.462729   \n",
      "0   1660006.0  4349447.0  0.542777  0.000109  0.003670  18396.0  0.178805   \n",
      "0    581111.0  1882617.0  0.000000  0.000000  0.000000    966.0  0.000000   \n",
      "..        ...        ...       ...       ...       ...      ...       ...   \n",
      "0   1461386.0  2341683.0  0.000000  0.000000  0.000000    116.0  0.000000   \n",
      "0   4057755.0  1871227.0  0.547422  0.000195  0.006061   5125.0  0.100722   \n",
      "0   4242514.0  1413468.0  0.000000  0.000000  0.000000     58.0  0.000000   \n",
      "0    555531.0  1290080.0  0.000000  0.000000  0.000000    171.0  0.000000   \n",
      "0   1707829.0  2373045.0  0.000000  0.000000  0.000000    502.0  0.000000   \n",
      "\n",
      "           7  \n",
      "0   0.000000  \n",
      "0   0.000000  \n",
      "0   0.004624  \n",
      "0   0.000028  \n",
      "0   0.000000  \n",
      "..       ...  \n",
      "0   0.000000  \n",
      "0   0.000049  \n",
      "0   0.000000  \n",
      "0   0.000000  \n",
      "0   0.000000  \n",
      "\n",
      "[2000 rows x 8 columns]\n",
      "           2         3         4        5         6         7\n",
      "0   0.000000  0.000000  0.000000    667.0  0.000000  0.000000\n",
      "0   0.000000  0.000000  0.000000    666.0  0.000000  0.000000\n",
      "0   0.626606  0.000692  0.011152   4335.0  0.462729  0.004624\n",
      "0   0.542777  0.000109  0.003670  18396.0  0.178805  0.000028\n",
      "0   0.000000  0.000000  0.000000    966.0  0.000000  0.000000\n",
      "..       ...       ...       ...      ...       ...       ...\n",
      "0   0.000000  0.000000  0.000000    116.0  0.000000  0.000000\n",
      "0   0.547422  0.000195  0.006061   5125.0  0.100722  0.000049\n",
      "0   0.000000  0.000000  0.000000     58.0  0.000000  0.000000\n",
      "0   0.000000  0.000000  0.000000    171.0  0.000000  0.000000\n",
      "0   0.000000  0.000000  0.000000    502.0  0.000000  0.000000\n",
      "\n",
      "[2000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test_df)\n",
    "test_df = test_df.iloc[:,2:20]\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_training_data_df = rescale_min_max(final_training_data_df)\n",
    "X=final_training_data_df\n",
    "# count=0\n",
    "# get the data and label\n",
    "y=final_labels_df\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_t, X_test, y_t, y_test = train_test_split(X,y)\n",
    "x_train = X_t\n",
    "x_test = X_test\n",
    "y_train = y_t\n",
    "y_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           3             4         5           6         7         8\n",
      "0   0.482961  4.476697e-06  0.000058   1116895.0  0.522342  0.001005\n",
      "0   0.576825  1.385580e-05  0.001066   3392081.0  4.323132  0.001132\n",
      "0   0.000000  0.000000e+00  0.000000    762657.0  0.000000  0.000000\n",
      "0   0.502149  7.063964e-06  0.000064    990945.0  0.628831  0.000179\n",
      "0   0.656400  5.056180e-04  0.018908     17800.0  1.667147  0.050839\n",
      "..       ...           ...       ...         ...       ...       ...\n",
      "0   0.000000  0.000000e+00  0.000000    101972.0  0.000000  0.000000\n",
      "0   0.497383  8.172178e-06  0.000049    611832.0  0.436495  0.000054\n",
      "0   0.496484  9.599897e-07  0.000054  42708792.0  3.840500  0.001287\n",
      "0   0.000000  1.530918e-05  0.003498    718523.0  0.983657  0.000302\n",
      "0   0.000000  0.000000e+00  0.000000        78.0  0.000000  0.000000\n",
      "\n",
      "[150000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1172/1172 [==============================] - 1s 877us/step - loss: 0.7027 - accuracy: 0.4988\n",
      "Epoch 2/100\n",
      "1172/1172 [==============================] - 1s 773us/step - loss: 0.6933 - accuracy: 0.5053\n",
      "Epoch 3/100\n",
      "1172/1172 [==============================] - 1s 770us/step - loss: 0.6905 - accuracy: 0.5287\n",
      "Epoch 4/100\n",
      "1172/1172 [==============================] - 1s 772us/step - loss: 0.6819 - accuracy: 0.5665\n",
      "Epoch 5/100\n",
      "1172/1172 [==============================] - 1s 780us/step - loss: 0.6688 - accuracy: 0.5995\n",
      "Epoch 6/100\n",
      "1172/1172 [==============================] - 1s 787us/step - loss: 0.6558 - accuracy: 0.6243\n",
      "Epoch 7/100\n",
      "1172/1172 [==============================] - 1s 789us/step - loss: 0.6603 - accuracy: 0.6119\n",
      "Epoch 8/100\n",
      "1172/1172 [==============================] - 1s 790us/step - loss: 0.6848 - accuracy: 0.5506\n",
      "Epoch 9/100\n",
      "1172/1172 [==============================] - 1s 795us/step - loss: 0.7043 - accuracy: 0.4913\n",
      "Epoch 10/100\n",
      "1172/1172 [==============================] - 1s 793us/step - loss: 0.7029 - accuracy: 0.4927\n",
      "Epoch 11/100\n",
      "1172/1172 [==============================] - 1s 793us/step - loss: 0.7024 - accuracy: 0.4868\n",
      "Epoch 12/100\n",
      "1172/1172 [==============================] - 1s 791us/step - loss: 0.7184 - accuracy: 0.4971\n",
      "Epoch 13/100\n",
      "1172/1172 [==============================] - 1s 792us/step - loss: 0.7348 - accuracy: 0.4888\n",
      "Epoch 14/100\n",
      "1172/1172 [==============================] - 1s 852us/step - loss: 0.7013 - accuracy: 0.4905\n",
      "Epoch 15/100\n",
      "1172/1172 [==============================] - 1s 925us/step - loss: 0.7005 - accuracy: 0.48740s - loss: 0.7006 - accu\n",
      "Epoch 16/100\n",
      "1172/1172 [==============================] - 1s 870us/step - loss: 0.7039 - accuracy: 0.4845\n",
      "Epoch 17/100\n",
      "1172/1172 [==============================] - 1s 931us/step - loss: 0.7097 - accuracy: 0.4880\n",
      "Epoch 18/100\n",
      "1172/1172 [==============================] - 1s 940us/step - loss: 0.6980 - accuracy: 0.4885\n",
      "Epoch 19/100\n",
      "1172/1172 [==============================] - 1s 898us/step - loss: 0.6952 - accuracy: 0.4976\n",
      "Epoch 20/100\n",
      "1172/1172 [==============================] - 1s 918us/step - loss: 0.6961 - accuracy: 0.4990\n",
      "Epoch 21/100\n",
      "1172/1172 [==============================] - 1s 861us/step - loss: 0.6970 - accuracy: 0.4976\n",
      "Epoch 22/100\n",
      "1172/1172 [==============================] - 1s 899us/step - loss: 0.6985 - accuracy: 0.4940\n",
      "Epoch 23/100\n",
      "1172/1172 [==============================] - 1s 931us/step - loss: 0.7005 - accuracy: 0.4946\n",
      "Epoch 24/100\n",
      "1172/1172 [==============================] - 1s 874us/step - loss: 0.6960 - accuracy: 0.4977\n",
      "Epoch 25/100\n",
      "1172/1172 [==============================] - 1s 833us/step - loss: 0.6953 - accuracy: 0.4998\n",
      "Epoch 26/100\n",
      "1172/1172 [==============================] - 1s 830us/step - loss: 0.6950 - accuracy: 0.5009\n",
      "Epoch 27/100\n",
      "1172/1172 [==============================] - 1s 813us/step - loss: 0.6955 - accuracy: 0.4973\n",
      "Epoch 28/100\n",
      "1172/1172 [==============================] - 1s 814us/step - loss: 0.6951 - accuracy: 0.4983\n",
      "Epoch 29/100\n",
      "1172/1172 [==============================] - 1s 860us/step - loss: 0.6960 - accuracy: 0.4996\n",
      "Epoch 30/100\n",
      "1172/1172 [==============================] - 1s 875us/step - loss: 0.6972 - accuracy: 0.4941\n",
      "Epoch 31/100\n",
      "1172/1172 [==============================] - 1s 880us/step - loss: 0.6974 - accuracy: 0.4992\n",
      "Epoch 32/100\n",
      "1172/1172 [==============================] - 1s 904us/step - loss: 0.6967 - accuracy: 0.5031\n",
      "Epoch 33/100\n",
      "1172/1172 [==============================] - 1s 903us/step - loss: 0.6930 - accuracy: 0.5259\n",
      "Epoch 34/100\n",
      "1172/1172 [==============================] - 1s 895us/step - loss: 0.6865 - accuracy: 0.5436\n",
      "Epoch 35/100\n",
      "1172/1172 [==============================] - 1s 846us/step - loss: 0.6790 - accuracy: 0.5720\n",
      "Epoch 36/100\n",
      "1172/1172 [==============================] - 1s 811us/step - loss: 0.6749 - accuracy: 0.5825\n",
      "Epoch 37/100\n",
      "1172/1172 [==============================] - 1s 812us/step - loss: 0.6583 - accuracy: 0.6095\n",
      "Epoch 38/100\n",
      "1172/1172 [==============================] - 1s 787us/step - loss: 0.6273 - accuracy: 0.6586\n",
      "Epoch 39/100\n",
      "1172/1172 [==============================] - 1s 778us/step - loss: 0.5837 - accuracy: 0.7040\n",
      "Epoch 40/100\n",
      "1172/1172 [==============================] - 1s 843us/step - loss: 0.5901 - accuracy: 0.7094\n",
      "Epoch 41/100\n",
      "1172/1172 [==============================] - 1s 833us/step - loss: 0.6172 - accuracy: 0.6808\n",
      "Epoch 42/100\n",
      "1172/1172 [==============================] - 1s 833us/step - loss: 0.6518 - accuracy: 0.6248\n",
      "Epoch 43/100\n",
      "1172/1172 [==============================] - 1s 808us/step - loss: 0.6174 - accuracy: 0.6755\n",
      "Epoch 44/100\n",
      "1172/1172 [==============================] - 1s 790us/step - loss: 0.6181 - accuracy: 0.6793\n",
      "Epoch 45/100\n",
      "1172/1172 [==============================] - 1s 888us/step - loss: 0.6491 - accuracy: 0.6265\n",
      "Epoch 46/100\n",
      "1172/1172 [==============================] - 1s 847us/step - loss: 0.6212 - accuracy: 0.6644\n",
      "Epoch 47/100\n",
      "1172/1172 [==============================] - 1s 856us/step - loss: 0.6203 - accuracy: 0.6638\n",
      "Epoch 48/100\n",
      "1172/1172 [==============================] - 1s 869us/step - loss: 0.6204 - accuracy: 0.6588\n",
      "Epoch 49/100\n",
      "1172/1172 [==============================] - 1s 916us/step - loss: 0.6122 - accuracy: 0.6680\n",
      "Epoch 50/100\n",
      "1172/1172 [==============================] - 1s 872us/step - loss: 0.5699 - accuracy: 0.7158\n",
      "Epoch 51/100\n",
      "1172/1172 [==============================] - 1s 804us/step - loss: 0.5569 - accuracy: 0.7336\n",
      "Epoch 52/100\n",
      "1172/1172 [==============================] - 1s 795us/step - loss: 0.5483 - accuracy: 0.7394\n",
      "Epoch 53/100\n",
      "1172/1172 [==============================] - 1s 791us/step - loss: 0.5679 - accuracy: 0.7152\n",
      "Epoch 54/100\n",
      "1172/1172 [==============================] - 1s 787us/step - loss: 0.5612 - accuracy: 0.7144\n",
      "Epoch 55/100\n",
      "1172/1172 [==============================] - 1s 790us/step - loss: 0.5402 - accuracy: 0.7403\n",
      "Epoch 56/100\n",
      "1172/1172 [==============================] - 1s 800us/step - loss: 0.5515 - accuracy: 0.7292\n",
      "Epoch 57/100\n",
      "1172/1172 [==============================] - 1s 789us/step - loss: 0.5481 - accuracy: 0.7333\n",
      "Epoch 58/100\n",
      "1172/1172 [==============================] - 1s 814us/step - loss: 0.5335 - accuracy: 0.7491\n",
      "Epoch 59/100\n",
      "1172/1172 [==============================] - 1s 821us/step - loss: 0.5413 - accuracy: 0.7409\n",
      "Epoch 60/100\n",
      "1172/1172 [==============================] - 1s 810us/step - loss: 0.5584 - accuracy: 0.7292\n",
      "Epoch 61/100\n",
      "1172/1172 [==============================] - 1s 832us/step - loss: 0.5443 - accuracy: 0.7477\n",
      "Epoch 62/100\n",
      "1172/1172 [==============================] - 1s 832us/step - loss: 0.5330 - accuracy: 0.7565\n",
      "Epoch 63/100\n",
      "1172/1172 [==============================] - 1s 825us/step - loss: 0.5343 - accuracy: 0.7553\n",
      "Epoch 64/100\n",
      "1172/1172 [==============================] - 1s 843us/step - loss: 0.5162 - accuracy: 0.7701\n",
      "Epoch 65/100\n",
      "1172/1172 [==============================] - 1s 834us/step - loss: 0.5540 - accuracy: 0.7430\n",
      "Epoch 66/100\n",
      "1172/1172 [==============================] - 1s 899us/step - loss: 0.5158 - accuracy: 0.7719\n",
      "Epoch 67/100\n",
      "1172/1172 [==============================] - 1s 818us/step - loss: 0.4911 - accuracy: 0.7860\n",
      "Epoch 68/100\n",
      "1172/1172 [==============================] - 1s 821us/step - loss: 0.4694 - accuracy: 0.7985\n",
      "Epoch 69/100\n",
      "1172/1172 [==============================] - 1s 811us/step - loss: 0.4778 - accuracy: 0.7944\n",
      "Epoch 70/100\n",
      "1172/1172 [==============================] - 1s 801us/step - loss: 0.4701 - accuracy: 0.7993\n",
      "Epoch 71/100\n",
      "1172/1172 [==============================] - 1s 818us/step - loss: 0.4615 - accuracy: 0.8032\n",
      "Epoch 72/100\n",
      "1172/1172 [==============================] - 1s 805us/step - loss: 0.4657 - accuracy: 0.8014\n",
      "Epoch 73/100\n",
      "1172/1172 [==============================] - 1s 812us/step - loss: 0.4615 - accuracy: 0.8044\n",
      "Epoch 74/100\n",
      "1172/1172 [==============================] - 1s 808us/step - loss: 0.4691 - accuracy: 0.7993\n",
      "Epoch 75/100\n",
      "1172/1172 [==============================] - 1s 815us/step - loss: 0.4751 - accuracy: 0.7980\n",
      "Epoch 76/100\n",
      "1172/1172 [==============================] - 1s 824us/step - loss: 0.4811 - accuracy: 0.7926\n",
      "Epoch 77/100\n",
      "1172/1172 [==============================] - 1s 812us/step - loss: 0.4595 - accuracy: 0.8059\n",
      "Epoch 78/100\n",
      "1172/1172 [==============================] - 1s 814us/step - loss: 0.4618 - accuracy: 0.8037\n",
      "Epoch 79/100\n",
      "1172/1172 [==============================] - 1s 847us/step - loss: 0.4617 - accuracy: 0.8043\n",
      "Epoch 80/100\n",
      "1172/1172 [==============================] - 1s 830us/step - loss: 0.4548 - accuracy: 0.8077\n",
      "Epoch 81/100\n",
      "1172/1172 [==============================] - 1s 837us/step - loss: 0.4538 - accuracy: 0.8081\n",
      "Epoch 82/100\n",
      "1172/1172 [==============================] - 1s 850us/step - loss: 0.4590 - accuracy: 0.8057\n",
      "Epoch 83/100\n",
      "1172/1172 [==============================] - 1s 853us/step - loss: 0.4961 - accuracy: 0.7899\n",
      "Epoch 84/100\n",
      "1172/1172 [==============================] - 1s 835us/step - loss: 0.4799 - accuracy: 0.7945\n",
      "Epoch 85/100\n",
      "1172/1172 [==============================] - 1s 803us/step - loss: 0.4717 - accuracy: 0.7989\n",
      "Epoch 86/100\n",
      "1172/1172 [==============================] - 1s 826us/step - loss: 0.4701 - accuracy: 0.7996\n",
      "Epoch 87/100\n",
      "1172/1172 [==============================] - 1s 820us/step - loss: 0.4476 - accuracy: 0.8117\n",
      "Epoch 88/100\n",
      "1172/1172 [==============================] - 1s 795us/step - loss: 0.4428 - accuracy: 0.8143\n",
      "Epoch 89/100\n",
      "1172/1172 [==============================] - 1s 793us/step - loss: 0.4501 - accuracy: 0.8110\n",
      "Epoch 90/100\n",
      "1172/1172 [==============================] - 1s 796us/step - loss: 0.4421 - accuracy: 0.8147\n",
      "Epoch 91/100\n",
      "1172/1172 [==============================] - 1s 798us/step - loss: 0.4647 - accuracy: 0.8034\n",
      "Epoch 92/100\n",
      "1172/1172 [==============================] - 1s 796us/step - loss: 0.4676 - accuracy: 0.8010\n",
      "Epoch 93/100\n",
      "1172/1172 [==============================] - 1s 822us/step - loss: 0.4606 - accuracy: 0.8038\n",
      "Epoch 94/100\n",
      "1172/1172 [==============================] - 1s 806us/step - loss: 0.4498 - accuracy: 0.8096\n",
      "Epoch 95/100\n",
      "1172/1172 [==============================] - 1s 805us/step - loss: 0.4397 - accuracy: 0.8164\n",
      "Epoch 96/100\n",
      "1172/1172 [==============================] - 1s 792us/step - loss: 0.4366 - accuracy: 0.8182\n",
      "Epoch 97/100\n",
      "1172/1172 [==============================] - 1s 816us/step - loss: 0.4343 - accuracy: 0.8198\n",
      "Epoch 98/100\n",
      "1172/1172 [==============================] - 1s 847us/step - loss: 0.4419 - accuracy: 0.8153\n",
      "Epoch 99/100\n",
      "1172/1172 [==============================] - 1s 833us/step - loss: 0.4370 - accuracy: 0.8181\n",
      "Epoch 100/100\n",
      "1172/1172 [==============================] - 1s 803us/step - loss: 0.4362 - accuracy: 0.8165\n",
      "391/391 [==============================] - 0s 439us/step - loss: 0.3847 - accuracy: 0.9061\n"
     ]
    }
   ],
   "source": [
    "# x_train = np.random.random((1000, 20))\n",
    "# y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "# x_test = np.random.random((100, 20))\n",
    "# y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=6, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=100,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape\n",
    "test_data_df = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:48, 41.39it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    make the prediction using the jaccard's coefficient\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    cnt=0\n",
    "    for row in tqdm(test_data_df.iterrows()):\n",
    "        row_df = pd.DataFrame(row[1]).T\n",
    "        single_result = model.predict(row_df)[0][0]\n",
    "#         print(model.predict(row_df)[0][0])\n",
    "        cnt+=1\n",
    "        result.append((cnt, single_result))\n",
    "    return result\n",
    "result = predict()\n",
    "\n",
    "# # save the result\n",
    "\n",
    "\n",
    "'''\n",
    "Description: get time\n",
    "Input: \n",
    "Output: time\n",
    "''' \n",
    "def nowtime():\n",
    "    return time.strftime(\"%Y%m%d-%H%M\", time.localtime())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Description: Save prediction result to files\n",
    "Input: (1) result\n",
    "       (2) filename\n",
    "Output: \n",
    "\"\"\"\n",
    "def save_prediction_to_csv(result,filename):\n",
    "    headers = ['id','Predicted']\n",
    "\n",
    "    with open(filename + str(nowtime()) + \".csv\", 'w', encoding = 'utf8') as f:\n",
    "        f_csv = csv.writer(f)\n",
    "        f_csv.writerow(headers)\n",
    "        f_csv.writerows(result)\n",
    "save_prediction_to_csv(result, \"shawn_ann_v1_200epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
